# -*- coding: utf-8 -*-
"""Tennsorflow HelloWorld.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lO7TBDGET5F_iFn1j8k5NwB7SY0TW0e-
"""

import tensorflow as tf
import keras
import numpy as np

"""Simplest possible Neural Network is one having one neuron in it. And that's what below line of code does"""

# Dense is used to define layer of Connected Neurons. Here we have only one layer with 1 unit i.e. only a single neuron

# Succesive layers are defined in sequence hence the name Sequential()
# Define what's the Shape of input to the Neural Network in the first layer (in this case it is the only layer)

model = keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])

# loss functions and Optimizers in below line of code
# LOSS Func. measures How good or How bad it's Guess was, and gives the result to the optimizer which figures out the next guess
# Logic is: each guess should be better than the one before
# As the guesses gets better & better, and ACCURACY approaches 100%, the term Convergence is used

model.compile(optimizer = 'sgd', loss = 'mean_squared_error') # Stochastic Gradient Descent(sgd)

xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)
ys  = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)

"""**Training the Neural Network**"""

model.fit(xs, ys, epochs=500)

model.predict([10.0])
